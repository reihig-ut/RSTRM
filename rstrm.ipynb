{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rosenbrock(x::AbstractVector)\n",
    "    return sum(100 .* (x[2:end] .- x[1:end-1].^2).^2 .+ (1 .- x[1:end-1]).^2)\n",
    "end\n",
    "\n",
    "\n",
    "function rosenbrock_gradient(x::AbstractVector)\n",
    "    n = length(x)\n",
    "    grad = zeros(n)\n",
    "    grad[1] = -400 * x[1] * (x[2] - x[1]^2) + 2 * (x[1] - 1)\n",
    "    for i in 2:n-1\n",
    "        grad[i] = 200 * (x[i] - x[i-1]^2) - 400 * x[i] * (x[i+1] - x[i]^2) + 2 * (x[i] - 1)\n",
    "    end\n",
    "    grad[n] = 200 * (x[n] - x[n-1]^2)\n",
    "    return grad\n",
    "end\n",
    "\n",
    "\n",
    "function rosenbrock_hessian(x::AbstractVector)\n",
    "    n = length(x)\n",
    "    hessian = zeros(n, n)\n",
    "    hessian[1, 1] = 1200*x[1]^2 - 400*x[2] + 2\n",
    "    for i in 1:n-1\n",
    "        hessian[i, i+1] = -400*x[i]\n",
    "        hessian[i+1, i] = -400*x[i]\n",
    "    end\n",
    "    for i in 2:n-1\n",
    "        hessian[i, i] = 202 + 1200*x[i]^2 - 400*x[i+1]\n",
    "    end\n",
    "    hessian[n, n] = 200\n",
    "    return hessian\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(1234)\n",
    "\n",
    "function generate_random_gaussian_matrix(s::Int, n::Int)\n",
    "    P = randn(rng, Float64, (s, n))\n",
    "    return P ./ sqrt(s)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function solve_subproblem(g::Array{Float64}, H::Array{Float64}, delta::Float64, n::Int)\n",
    "    F = [H g; g' -delta]\n",
    "    vals, vecs, info = KrylovKit.eigsolve(\n",
    "            F, n + 1, 1, :SR, Float64;\n",
    "            issymmetric=true, tol=1e-12\n",
    "        )\n",
    "    return vecs[1]\n",
    "end\n",
    "\n",
    "\n",
    "function calc_stepsize_with_ls(linesearch, loss, der, x, d)\n",
    "    φ(eta) = loss(x .+ eta .* d)\n",
    "    function dφ(eta)\n",
    "        return dot(der(x .+ eta .* d), d)\n",
    "    end\n",
    "    function φdφ(eta)\n",
    "        return (φ(eta), dφ(eta))\n",
    "    end\n",
    "\n",
    "    fx = loss(x)\n",
    "    gx = der(x)\n",
    "    dφ0 = dot(d, gx)\n",
    "    try\n",
    "        eta, fx = linesearch(φ, dφ, φdφ, 1.0, fx, dφ0)\n",
    "        return eta\n",
    "    catch e\n",
    "        println(\"Line search failed: \", e)\n",
    "        return 1.0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSODM\n",
    "function HSODM_linesearch(\n",
    "    x::Array{Float64},\n",
    "    loss::Function,\n",
    "    der::Function,\n",
    "    hes::Function,\n",
    "    nu::Float64,\n",
    "    Delta::Float64,\n",
    "    delta::Float64,\n",
    "    n::Int,\n",
    "    maxiter::Int,\n",
    "    eps_grad::Float64,\n",
    "    linesearch=LineSearches.HagerZhang()\n",
    ")\n",
    "    xs = []\n",
    "    timestamps = []\n",
    "    obj_vals = []\n",
    "    grad_norms = []\n",
    "    elapsed_time = 0.0\n",
    "    terminate = false\n",
    "\n",
    "    push!(xs, x)\n",
    "    push!(timestamps, elapsed_time)\n",
    "    push!(obj_vals, loss(x))\n",
    "    push!(grad_norms, norm(der(x)))\n",
    "\n",
    "    @showprogress dt=1 desc=\"HSODM_linesearch\" for i in 1:maxiter\n",
    "        runtime = @elapsed begin\n",
    "            g::Array{Float64} = der(x)\n",
    "            H::Array{Float64} = Symmetric(hes(x))\n",
    "            subprob_sol::Array{Float64} = solve_subproblem(g, H, delta, n)\n",
    "            v::Array{Float64} = subprob_sol[begin:end-1]\n",
    "            t::Float64 = subprob_sol[end]\n",
    "\n",
    "            if abs(t) >= nu\n",
    "                d = v ./ t\n",
    "            else\n",
    "                sign = ifelse(- dot(g, v) > 0, 1, -1)\n",
    "                d = sign .* v\n",
    "            end\n",
    "\n",
    "            norm_d = norm(d)\n",
    "            if norm_d > Delta\n",
    "                eta = calc_stepsize_with_ls(linesearch, loss, der, x, d)\n",
    "                x = x .+ eta .* d\n",
    "            else\n",
    "                x += d\n",
    "                println(\"norm_d <= Delta\")\n",
    "                terminate = true\n",
    "            end\n",
    "        end\n",
    "\n",
    "        elapsed_time += runtime\n",
    "\n",
    "        push!(xs, x)\n",
    "        push!(timestamps, elapsed_time)\n",
    "        push!(obj_vals, loss(x))\n",
    "        push!(grad_norms, norm(der(x)))\n",
    "\n",
    "        if grad_norms[end] < eps_grad\n",
    "            println(\"grad_norms[end] < eps_grad\")\n",
    "            terminate = true\n",
    "        end\n",
    "\n",
    "        if terminate\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"Norm of gradient: \\t\\t\\t\\t\\t\", norm(der(x)))\n",
    "\n",
    "    return xs, timestamps, obj_vals, grad_norms\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function RSTRM_linesearch(\n",
    "    x::Array{Float64, 1},\n",
    "    loss::Function,\n",
    "    der::Function,\n",
    "    hes::Function,\n",
    "    nu::Float64,\n",
    "    Delta::Float64,\n",
    "    delta::Float64,\n",
    "    n::Int,\n",
    "    s::Int,\n",
    "    max_iter::Int,\n",
    "    eps_grad::Float64,\n",
    "    linesearch=LineSearches.BackTracking()\n",
    ")\n",
    "    xs = []\n",
    "    timestamps = []\n",
    "    obj_vals = []\n",
    "    grad_norms = []\n",
    "    count = 0\n",
    "    iter = 0\n",
    "    terminate = false\n",
    "    elapsed_time = 0.0\n",
    "\n",
    "    push!(xs, x)\n",
    "    push!(timestamps, elapsed_time)\n",
    "    push!(obj_vals, loss(x))\n",
    "    push!(grad_norms, norm(der(x)))\n",
    "\n",
    "\n",
    "    @showprogress dt=1 desc=\"RSTRM_linesearch (s=$s)\" for i in 1:max_iter\n",
    "        runtime = @elapsed begin\n",
    "            g::Array{Float64} = der(x)\n",
    "            H::Array{Float64} = Symmetric(hes(x))\n",
    "            P::Array{Float64} = generate_random_gaussian_matrix(s, n)\n",
    "            PT::Array{Float64} = P'\n",
    "            g_sub = P * g\n",
    "            H_sub = P * H * PT\n",
    "\n",
    "            subprob_sol::Array{Float64} = solve_subproblem(g_sub, H_sub, delta, s)\n",
    "            v_sub::Array{Float64} = subprob_sol[begin:end-1]\n",
    "            t::Float64 = subprob_sol[end]\n",
    "\n",
    "            if abs(t) > nu\n",
    "                d = PT * v_sub ./ t\n",
    "            else\n",
    "                sign = ifelse(- dot(g_sub, v_sub) > 0, 1, -1)\n",
    "                d = sign .* PT * v_sub\n",
    "            end\n",
    "\n",
    "            norm_d = norm(d)\n",
    "            if norm_d > Delta\n",
    "                eta = calc_stepsize_with_ls(linesearch, loss, der, x, d)\n",
    "                x = x .+ eta .* d\n",
    "            else\n",
    "                x += d\n",
    "                println(\"norm_d <= Delta\")\n",
    "                terminate = true\n",
    "            end\n",
    "        end\n",
    "        elapsed_time += runtime\n",
    "\n",
    "        push!(xs, x)\n",
    "        push!(timestamps, elapsed_time)\n",
    "        push!(obj_vals, loss(x))\n",
    "        push!(grad_norms, norm(der(x)))\n",
    "\n",
    "        if grad_norms[end] < eps_grad\n",
    "            println(\"grad_norm[end] < eps_grad\")\n",
    "            terminate = true\n",
    "        end\n",
    "\n",
    "        if terminate\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"Norm of gradient: \", norm(der(x)))\n",
    "\n",
    "    return xs, timestamps, obj_vals, grad_norms\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function GD_linesearch(\n",
    "    x::Array{Float64},\n",
    "    loss::Function,\n",
    "    der::Function,\n",
    "    nu::Float64,\n",
    "    maxiter::Int,\n",
    "    eps_grad::Float64,\n",
    "    linesearch=LineSearches.HagerZhang()\n",
    ")\n",
    "    xs = []\n",
    "    timestamps = []\n",
    "    obj_vals = []\n",
    "    grad_norms = []\n",
    "    elapsed_time = 0.0\n",
    "    terminate = false\n",
    "\n",
    "    push!(xs, x)\n",
    "    push!(timestamps, elapsed_time)\n",
    "    push!(obj_vals, loss(x))\n",
    "    push!(grad_norms, norm(der(x)))\n",
    "\n",
    "    @showprogress dt=1 desc=\"GD_linesearch\" for i in 1:maxiter\n",
    "        runtime = @elapsed begin\n",
    "            g::Array{Float64} = der(x)\n",
    "            d = -g\n",
    "            eta = calc_stepsize_with_ls(linesearch, loss, der, x, d)\n",
    "            x = x .+ eta .* d\n",
    "        end\n",
    "\n",
    "        elapsed_time += runtime\n",
    "\n",
    "        push!(xs, x)\n",
    "        push!(timestamps, elapsed_time)\n",
    "        push!(obj_vals, loss(x))\n",
    "        push!(grad_norms, norm(der(x)))\n",
    "\n",
    "        if grad_norms[end] < eps_grad\n",
    "            terminate = true\n",
    "        end\n",
    "\n",
    "        if terminate\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"Norm of gradient: \\t\\t\\t\\t\\t\", norm(der(x)))\n",
    "\n",
    "    return xs, timestamps, obj_vals, grad_norms\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "r = 25\n",
    "s_list = [25, 50, 75]\n",
    "nu = 0.1\n",
    "eps = 1e-8\n",
    "Delta = eps^(1/2) \n",
    "delta = eps^(1/2)  # * (1 + xi_1)^2 * n / s in RSTRM\n",
    "max_iter = 20000  # * 2 in GD\n",
    "eps_grad = 0.0\n",
    "\n",
    "Random.seed!(1234)\n",
    "x0 = randn(n)\n",
    "x_opt = ones(n)\n",
    "R = generate_random_gaussian_matrix(r, n)\n",
    "loss(x) = rosenbrock_rank_deficient(x, R)\n",
    "der(x) = rosenbrock_rank_deficient_gradient(x, R)\n",
    "hes(x) = rosenbrock_rank_deficient_hessian(x, R)\n",
    "\n",
    "\n",
    "# HSODM\n",
    "xs, timestamps, obj_vals, grad_norms = HSODM_linesearch(\n",
    "    copy(x0), loss, der, hes,\n",
    "    nu, Delta, delta, n, max_iter, eps_grad\n",
    ")\n",
    "plot_obj_vals_vs_iter = plot(\n",
    "    obj_vals,\n",
    "    xlabel=\"Iteration\",\n",
    "    ylabel=\"Objective function value\",\n",
    "    lw=2,\n",
    "    yscale=:log10,\n",
    "    label=\"HSODM (n=$n)\"\n",
    ")\n",
    "plot_obj_vals_vs_time = plot(\n",
    "    timestamps,\n",
    "    obj_vals,\n",
    "    xlabel=\"Time (s)\",\n",
    "    ylabel=\"Objective function value\",\n",
    "    lw=2,\n",
    "    yscale=:log10,\n",
    "    label=\"HSODM (n=$n)\"\n",
    ")\n",
    "plot_grad_norms_vs_iter = plot(\n",
    "    grad_norms,\n",
    "    xlabel=\"Iteration\",\n",
    "    ylabel=\"Norm of gradient\",\n",
    "    lw=2,\n",
    "    yscale=:log10,\n",
    "    label=\"HSODM (n=$n)\"\n",
    ")\n",
    "plot_grad_norms_vs_time = plot(\n",
    "    timestamps,\n",
    "    grad_norms,\n",
    "    xlabel=\"Time (s)\",\n",
    "    ylabel=\"Norm of gradient\",\n",
    "    lw=2,\n",
    "    yscale=:log10,\n",
    "    label=\"HSODM (n=$n)\"\n",
    ")\n",
    "\n",
    "\n",
    "# GD\n",
    "xs, timestamps, obj_vals, grad_norms = GD_linesearch(\n",
    "    copy(x0), loss, der, nu, 2 * max_iter, eps_grad\n",
    ")\n",
    "plot!(\n",
    "    plot_obj_vals_vs_iter,\n",
    "    obj_vals,\n",
    "    lw=2,\n",
    "    yscale=:log10,\n",
    "    label=\"GD (n=$n)\"\n",
    ")\n",
    "plot!(\n",
    "    plot_obj_vals_vs_time,\n",
    "    timestamps,\n",
    "    obj_vals,\n",
    "    lw=2,\n",
    "    yscale=:log10,\n",
    "    label=\"GD (n=$n)\"\n",
    ")\n",
    "plot!(\n",
    "    plot_grad_norms_vs_iter,\n",
    "    grad_norms,\n",
    "    lw=2,\n",
    "    yscale=:log10,\n",
    "    label=\"GD (n=$n)\"\n",
    ")\n",
    "plot!(\n",
    "    plot_grad_norms_vs_time,\n",
    "    timestamps,\n",
    "    grad_norms,\n",
    "    lw=2,\n",
    "    yscale=:log10,\n",
    "    label=\"GD (n=$n)\"\n",
    ")\n",
    "\n",
    "\n",
    "# RSTRM\n",
    "for s in s_list\n",
    "    xi_1 = sqrt(s/n)\n",
    "    xs, timestamps, obj_vals, grad_norms = RSTRM_linesearch(\n",
    "        copy(x0), loss, der, hes,\n",
    "        nu, Delta, delta * ((1 + xi_1)^2) * n / s, n, s, max_iter, eps_grad\n",
    "    )\n",
    "    plot!(\n",
    "        plot_obj_vals_vs_iter,\n",
    "        obj_vals,\n",
    "        lw=2,\n",
    "        yscale=:log10,\n",
    "        label=\"RSTRM (s=$s)\"\n",
    "    )\n",
    "    plot!(\n",
    "        plot_obj_vals_vs_time,\n",
    "        timestamps,\n",
    "        obj_vals,\n",
    "        lw=2,\n",
    "        yscale=:log10,\n",
    "        label=\"RSTRM (s=$s)\"\n",
    "    )\n",
    "    plot!(\n",
    "        plot_grad_norms_vs_iter,\n",
    "        grad_norms,\n",
    "        lw=2,\n",
    "        yscale=:log10,\n",
    "        label=\"RSTRM (s=$s)\"\n",
    "    )\n",
    "    plot!(\n",
    "        plot_grad_norms_vs_time,\n",
    "        timestamps,\n",
    "        grad_norms,\n",
    "        lw=2,\n",
    "        yscale=:log10,\n",
    "        label=\"RSTRM (s=$s)\"\n",
    "    )\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot!(plot_obj_vals_vs_iter, xlim=(0,100), ylim=(1e-8, 1e8), legendfontsize=10)\n",
    "display(plot_obj_vals_vs_iter)\n",
    "\n",
    "plot!(plot_obj_vals_vs_time, xlim=(0,5), ylim=(1e-8,1e8), legendfontsize=10)\n",
    "display(plot_obj_vals_vs_time)\n",
    "\n",
    "plot!(plot_grad_norms_vs_iter, xlim=(0,100), ylim=(1e-8, 1e8), legendfontsize=10)\n",
    "display(plot_grad_norms_vs_iter)\n",
    "\n",
    "plot!(plot_grad_norms_vs_time, xlim=(0,5), ylim=(1e-8,1e8), legendfontsize=10)\n",
    "display(plot_grad_norms_vs_time)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
